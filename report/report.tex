% !TEX TS-program = xelatex
% !TEX encoding = UTF-8

\documentclass[a4paper, 11pt, twocolumn, final]{article} % use larger type; default would be 10pt

\usepackage{default}

\defaultfontfeatures{Mapping=tex-text}
\setmainfont[Ligatures=TeX]{Linux Libertine}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{bm}

\usepackage{chngcntr}
\counterwithin{table}{section}
\counterwithin{figure}{section}
\counterwithin{equation}{section}

\title{Reinforcement learning with Sarsa, Sarsa(λ) and a Neural Network}
\author{Student 120133998, University of Sheffield}
\date{\today}

\begin{document}

\twocolumn[{
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}

    This report outlines our investigations into the reinforcement learning
    problem by using an example `homing' task.  We apply the Sarsa algorithm and
    use an artificial neural network to learn Q-values, and then implement
    various extensions to the Sarsa algorithm including Sarsa(λ).  We compare
    these extensions success at learning the homing problem and tune their
    parameters.

    \end{abstract}
    % \tableofcontents
    % \vspace{20pt}
  \end{@twocolumnfalse}
}]

\section{Introduction} This report is part of Assignment B for the module
COM3240: Adaptive Intelligence at the University of Sheffield.  Our code
implementation was done using Python with the NumPy \cite{NumPy} library, and
our theoretical work is mostly taken from Sutton \& Barto's foundational book on
reinforcement learning \cite{Sutton1998}.

All figures in this report can be generated by running the correspondingly-named
functions in the \texttt{driver.py} file in the attached code listings, and the
parameters used to generate them are listed in \autoref{tab:parameters}.

\section{Homing task} The task we chose to focus on was a simple goal-oriented
`homing' task: a robot is placed in a room at random, and has to make its way to
the charging station, which is at a fixed place in the room.  We modelled this
using a grid of $10\times10$ discrete positions, with the robot able to move in
the four compass directions in a single action, and its charging station placed
at position $(5, 5)$.

Each episode\footnote{Note that we choose to use the terminology of `episodes'
rather than `trials', as in \cite[p.~58]{Sutton1998}.} consists of the robot
making moves according to its policy $\pi$ until either a maximum number of
moves (here denoted $t_{max}$) is reached, or it reaches its charging station at
$(5, 5)$.

\section{Basic Sarsa algorithm} At first, we implement a basic Sarsa algorithm, as
outlined in \cite[Chapter~6]{Sutton1998} with an $\epsilon$-greedy policy.  We
follow their definition of `discounted return' (denoted $R_t$) as the sum of
expected rewards (denoted $r_t$) to be obtained under policy $\pi$, with each
future reward being geometrically discounted by a factor of $\gamma$, i.e.
\begin{align}
R_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots \nonumber \\
\text{or } R_t &= \sum_{i=0}^{\infty}{\gamma^ir_{t+i}} \\
&\text{where }0 \le \gamma < 1 \nonumber
\end{align}

(Note that in general we may relax the restriction to $0 \le \gamma \le 1$ if we
know that the sequence ($t_n$) eventually becomes 0.)

The `Q-value' of a particular state-action pair, denoted $Q^\pi(s, a)$ is the
expected discounted return after taking action $a$ when in state $s$ and
following policy $\pi$ thereafter.  The Sarsa algorithm attempts to solve the
reinforcement learning problem by estimating and refining these Q-values over
every step of a training episode; we will omit the bulk of the algorithm here
(it can be found at \cite[p.~146]{Sutton1998}), but note that the crucial step
is in updating the Q-values\footnote{This is given in a slightly different form
in \cite{Sutton1998}, but I feel this was is more demonstrative of a weighted
average of the current and `target' values.}:
\begin{equation}
Q(s, a) \leftarrow (1-\alpha)Q(s, a) + \alpha[r + \gamma Q(s', a')]
\end{equation}

$Q(s, a)$ can either be used and estimated directly, or can be generated from
some parametrised model.  Our basic approach estimates directly, but then in
\autoref{sec:ext_nn} we extend this model with an artificial neural network.

As for selecting an action, we used the $\epsilon$-greedy policy throughout this
investigation:\footnote{Although we tried implementing a softmax policy with a
Gibbs-Boltzmann distribution, we tended to agree with Sutton \& Barto that
estimating a good value for $\tau$ was difficult \cite[p.~31]{Sutton1998}, and
in any case the algorithm seemed to perform poorly with this method.}  With
probability $(1-\epsilon)$ we select the action with the best Q-value, as
estimated by Sarsa, and with proability $\epsilon$ we choose randomly between
the actions.  This allows a certain degree of `exploring' while maintaining a
general stability.

\section{Learning curves and evaluation} Before moving on to a more in-depth
discussion, we briefly define the term `learning curve': in the context of our
task, we will define it as the number of steps taken to achieve the goal (i.e.
move to position $(5, 5)$) per episode.  Since with each episode, the task is
expected to be performed increasingly more efficiently, i.e. that the number of
steps taken to reach the goal should be decreasing, we measure the effectiveness
of an algorithm or set of parameters by the steepness and eventual resting point
of the decreasing learning curve.

Presented in \autoref{fig:q1_single} are two example learning curves using the
$\epsilon$-greedy policy.  Since there is an element of randomness in
$\epsilon$-greedy, and also in the choice of initial position of the robot in
the environment, the learning curve itself is non-deterministic.  To mitigate
this uncertainty, we typically average our values over a number of runs; in this
investigation we denote this number $S$ and usually take $S = 100$.  In
\autoref{fig:q1_avg} is shown an average learning curve with the same parameters
as in \autoref{fig:q1_single}, with $S = 100$ and error bars shown.  All further
learning curve graphs in this report will be of averaged learning
curves.\footnote{This answers Question 1.}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/q1_figure_single.png}
  \caption{Two learning curves, 100-point moving average}
  \label{fig:q1_single}
\end{figure}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/q1_figure_avg.png}
  \caption{Average learning curve, sampled every 10 data points}
  \label{fig:q1_avg}
\end{figure}

\section{Extensions} In this section, we attempt to build on our established
Sarsa model with various extensions, including using an artificial neural
network (abbreviated as ANN hereafter) to generate Q-values.  Since using an ANN
was suggested in the assignment brief, we use the above basic learning model
\textbf{with} the ANN as our baseline against which we will measure the other
extensions.

\subsection{Neural network} \label{sec:ext_nn} For our ANN, we use a simple
1-layer model with no hidden neurons, where there is one input neuron per state
(i.e. per position in the room), and one output neuron for each action to take.
The synapse weights are adjusted as per the above Sarsa rule for adjusting the
Q-values.  Comparing this method with the basic method at first sight gives
disappointing results---the ANN-backed model actually performs substantially
worse than the basic model above (see
\autoref{fig:ext_basic_vs_nn_vs_eligibility}).  However, as we will see, when an
eligibility trace is added this situation is reversed.

\subsection{Eligibility trace (TD(λ))} Whereas the standard Sarsa algorithm
updates each value of $Q(s, a)$ for $s = s_t$ and for all $a \in \mathcal{A}_s$
only to be closer to the `target' value of $r + \gamma Q(s', a')$, if we add an
eligibility trace to this, the Q-values for \textit{all} states are adjusted
similarly, but in proportion to the time they were most recently visited (i.e.
their `eligibility').  When an eligibility trace is added to Sarsa in this way,
it becomes known as Sarsa(λ).

As suggested by the assignment brief, we implemented an eligibility trace to
both our basic model and our ANN-backed model.  The results were successful, as
can be seen again in \autoref{fig:ext_basic_vs_nn_vs_eligibility}.\footnote{This
answers Question 2.}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/basic_vs_nn_vs_eligibility.png}
  \caption{Learning curves for the basic model vs. ANN model vs. using an
  eligibility trace.}
  \label{fig:ext_basic_vs_nn_vs_eligibility}
\end{figure}

% TODO justify why?
\subsection{Decaying $\epsilon$} Another extension we implement is simply
letting the value of $\epsilon$ in the $\epsilon$-greedy policy decay over time.
We establish a rule, with $n$ being the episode number in the run, of
\begin{equation}
  \epsilon_n = \frac{\epsilon_0}{n}
\end{equation}
where $\epsilon_0$ is a parameter for the initial value of $\epsilon$.  As can
be seen in \autoref{fig:ext_epsilon_vs_decay}, this made little impact on the
learning curves.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/epsilon_decay.png}
  \caption{Learning curves for the $\epsilon$-greedy policy; plain vs. decaying
  $\epsilon$}
  \label{fig:ext_epsilon_vs_decay}
\end{figure}

\subsection{Extended actions} Finally, we allowed a further 4 movements to be
made in our environment, along the diagonals of the compass points.  This allows
for a diagonal move to be made in one action rather than two, and thus decreases
the minimum number of moves necessary to reach the goal by up to a half.

This effect can be clearly seen in \autoref{fig:ext_8_actions}, in which the
learning curve for the 8-action environment stabilises between $0.5\times$ and
$1\times$ the number of steps as the 4-action environment.\footnote{This answers
Question 5.}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/4_actions_vs_8_actions.png}
  \caption{Comparing a 4-action environment to an 8-action environment}
  \label{fig:ext_8_actions}
\end{figure}

\section{Parameter tuning} In this section we attempt to find optimal parameters
for our learning model.  Owing to the computational resources available to us,
we must assume that all parameters under consideration are independent.  As a
result of this assumption, when comparing different values of, say, $\epsilon$,
we may fix the other parameters arbitrarily, rather than having to compare our
chosen testing values of $\epsilon$ with every combination of values for the
other parameters.

The parameters we will be considering for our optimisation will be $\epsilon$
(and whether to use a decaying-$\epsilon$ model), the learning rate ($\alpha$),
the discount rate ($\gamma$), and the eligibility trace decay rate ($\lambda$).

First we compare values for $\alpha$, in \autoref{fig:tuning_alpha}.  As
expected, $\alpha = 0$ performs extremely badly; and the best value seems to be
somewhere in the range $\alpha \in [0.6, 1.0]$.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/tuning_alpha.png}
  \caption{Tuning the $\alpha$ parameter}
  \label{fig:tuning_alpha}
\end{figure}

Next, we compare values for $\gamma$, in \autoref{fig:tuning_gamma}.  The best
value seems to be $\gamma \approx 0.6$.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/tuning_gamma.png}
  \caption{Tuning the $\gamma$ parameter}
  \label{fig:tuning_gamma}
\end{figure}

We have already calculated the necessary curves to optimise $\epsilon$, in
\autoref{fig:ext_epsilon_vs_decay}.  It is difficult to tell what the optimal
value is, but it seems to be either $\epsilon = 0$ (i.e. a completely greedy
policy), or $\epsilon_t = \frac{1}{t}$.

Finally, we optimise $\lambda$, in \autoref{fig:tuning_lambda}.  In terms of the
final resting efficiency of the algorithm, it seems we are tied between $\lambda =
0.2, 0.4, 0.5$; however, when we take the initial steepness of the curve into
account, $\lambda = 0.5$ clearly wins.\footnote{This answers Question 3.}

\begin{figure}
  \includegraphics[width=0.49\textwidth]{figures/tuning_lambda.png}
  \caption{Tuning the $\lambda$ parameter for the eligibility trace}
  \label{fig:tuning_lambda}
\end{figure}

\subsection{Representation of weights} When using the state as input to the
neural network, we choose to represent it as a vector of zeros with a single 1
representing the state index: $[0, \ldots, 0, 1, 0, \ldots, 0]^T$.  Thus, fixing
a state $\sigma$, the weights connecting $\nu_\sigma$ (the input neuron) to each
of the four output neurons encode the Q-values directly.  Taking the largest of
these weights gives us the preferred action of the model when in state $\sigma$.
Thus, we may produce a diagram of state-transitions when following a greedy
policy, and this diagram is represented in
\autoref{tab:computed_directions}.\footnote{This answers Question 4.}

\section{Limits of the model} Returning to the model of the environment, in this
section we investigate the limits of such a discrete model (with four discrete
actions that transfer deterministically between 100 discrete states).

This model may not transfer well to the physical world of robotics, as it is
often difficult to control a robot's physical velocity and position with such
discrete steps.  For example, losses of calibration may occur which mean that
the state the robot expects to be in does not match its real position.
Additionally, we may waste energy moving in perpendicular lines and stopping to
make a right-angle turn between every state.

It is clear that as the resolution of our sectors increases, and the number of
actions available to us increases, our environmental model becomes in the limit
equivalent to the real world.  The problem with this, however is that as said
resolution increases, the training time (and indeed the memory requirements of
the decision model) necessary to make good decisions in every state increases
too.

One rudimentary solution to this would be to incorporate a velocity vector as
part of the state, as well as increasing the number of actions available to 8 or
16; and penalise abrupt changes of velocity proportionally to the energy
required to make them.  This would encourage behaviours that match our desired
real-world behaviours better.\footnote{This answers Question 6.}

We leave the implementation of such methods beyond the scope of this
investigation.\footnote{This relates to (but obviously does not answer) Question
7.}

\bibliographystyle{IEEEtran}
\bibliography{report}

\onecolumn \appendix

\section{Tables}
\begin{table}[H]
  \centering
  \begin{tabular}{c | c c c c c c c c c c c c}
    \hline
    & $|\mathcal{A}|$ & $S$ & $E$ & $t_{max}$ & $\epsilon$ decaying? &
      $\epsilon$ & $\alpha$ & $\gamma$ & Trace? & $\lambda$ & ANN? \\
    \hline
    \autoref{fig:q1_single} & 4 & N/A & 1000 & 20 & No & 0.001 & 0.1 & 0.1 &
      No & N/A & No \\
    \autoref{fig:q1_avg} & 4 & 100 & 1000 & 20 & No & 0.001 & 0.1 & 0.1 & No &
      N/A & No \\
    \autoref{fig:ext_basic_vs_nn_vs_eligibility} & 4 & 200 & 200 & 20 & No &
      0.2 & 0.8 & 0.6 & $*$ & $*$ & $*$ \\
    \autoref{fig:ext_epsilon_vs_decay} & 4 & 100 & 200 & 20 & $*$ & $*$ & 0.8 &
      0.6 & No & N/A & Yes \\
    \autoref{fig:ext_8_actions} & $*$ & 100 & 400 & 20 & No & 0.1 & 0.8 & 0.6
      & Yes & 0.5 & Yes \\
    \autoref{fig:tuning_alpha} & 4 & 200 & 200 & 20 & No & 0.1 & $*$ & 0.6 &
      Yes & 0.5 & Yes \\
    \autoref{fig:tuning_gamma} & 4 & 100 & 200 & 20 & No & 0.1 & 0.8 & $*$ &
      Yes & 0.5 & Yes \\
    \hline
  \end{tabular}
  \caption{Parameters for figures generated.}
  \label{tab:parameters}
\end{table}

\begin{itemize*}
  \item $|\mathcal{A}|$ denotes the number of actions available to the actor in
    the environment
  \item $S$ denotes the number of runs that the figure is averaged over
  \item $E$ denotes the number of episodes used in each run
  \item $t_{max}$ denotes the maximum number of steps in any episode
  \item `$\epsilon$ decaying?' denotes whether a decaying $\epsilon$-greedy
    policy was used
  \item $\epsilon$ is the parameter of the $\epsilon$-greedy model
  \item $\alpha$ is the learning rate
  \item $\gamma$ is the discount rate
  \item `Trace?' denotes whether an eligibility trace was used
  \item $\lambda$ is the trace decay rate
  \item `ANN?' denotes whether an artificial neural network was used
  \item $*$ is used when parameter varies within the figure
\end{itemize*}

\begin{table}[H]
  \centering
  \begin{tabular}{r | c c c c c c c c c c}
    \hline
      & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
    \hline
    0 & → & ↓ & ↓ & ↓ & → & ↓ & ↓ & ← & ← & ↓ \\
    1 & ↓ & ↓ & ↓ & ↓ & ↓ & ↓ & ↓ & ← & ↓ & ↓ \\
    2 & ↓ & ↓ & ↓ & ↓ & ↓ & ↓ & ↓ & ↓ & ← & ↓ \\
    3 & ↓ & → & ↓ & ← & ↓ & → & ↓ & ← & ↓ & ↓ \\
    4 & ↓ & ↓ & → & ↓ & ↓ & ↓ & ↓ & ↓ & ← & ← \\
    5 & → & → & → & → & → & \colorbox{pink}{↓} & ← & ← & ↑ & ← \\
    6 & ↑ & → & → & ↑ & ↑ & ↑ & ↑ & ↑ & ← & ← \\
    7 & → & → & ↑ & ← & ↑ & → & ↑ & ← & ← & ← \\
    8 & → & ↑ & ↑ & → & → & → & ↑ & ↑ & ← & ↑ \\
    9 & ↑ & ↑ & ↑ & → & ↑ & ↑ & ↑ & ← & ↑ & ↑ \\
    \hline
  \end{tabular}
  \caption{Best-choice actions for each state, as computed in one run of the
  algorithm.  The goal state is highlighted in pink.}
  \label{tab:computed_directions}
\end{table}

\section{Code snippets} \label{sec:code}
Please see the attached full code listings.

\end{document}
% Disable intented paragraphs and add space between them instead.
% \setlength{\parskip}{10pt}
% \setlength{\parindent}{0pt
