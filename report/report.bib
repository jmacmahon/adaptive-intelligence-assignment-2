@misc{Sutton1998,
abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. If you would like to order a copy of the book, or if you are qualified instructor and would like to see an examination copy, please see the MIT Press home page for this book. Or you might be interested in the reviews at amazon.com. There is also a Japanese translation available. The table of contents of the book is given below, with associated HTML. The HTML version has a number of presentation problems, and its text is slightly different from the real book, but it may be useful for some purposes. q Preface Part I: The Problem q 1 Introduction r 1.1 Reinforcement Learning r 1.2 Examples r 1.3 Elements of Reinforcement Learning r 1.4 An Extended Example: Tic-Tac-Toe r 1.5 Summary r 1.6 History of Reinforcement Learning r 1.7 Bibliographical Remarks q 2 Evaluative Feedback r 2.1 An n-armed Bandit Problem r 2.2 Action-Value Methods r 2.3 Softmax Action Selection r 2.4 Evaluation versus Instruction r 2.5 Incremental Implementation r 2.6 Tracking a Nonstationary Problem r 2.7 Optimistic Initial Values r 2.8 Reinforcement Comparison r 2.9 Pursuit Methods r 2.10 Associative Search r 2.11 Conclusion r 2.12 Bibliographical and Historical Remarks q 3 The Reinforcement Learning Problem r 3.1 The Agent-Environment Interface r 3.2 Goals and Rewards r 3.3 Returns r 3.4 A Unified Notation for Episodic and Continual Tasks r 3.5 The Markov Property r 3.6 Markov Decision Processes r 3.7 Value Functions r 3.8 Optimal Value Functions r 3.9 Optimality and Approximation r 3.10 Summary r 3.11 Bibliographical and Historical Remarks Part II: Elementary Methods},
author = {Sutton, Richard S. and Barto, Andrew G.},
booktitle = {MIT Press, Cambridge, MA, A Bradford Book},
keywords = {reinforcement learning theory},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
